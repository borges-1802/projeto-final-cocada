"""
Projeto COCADA 2025-2: An√°lise de Atrasos - Linhas do Fund√£o
Jo√£o Victor Borges Nascimento - 121064604

Usando Base dos Dados (basedosdados) para acesso aos dados
"""

import basedosdados as bd
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Configurar estilo dos gr√°ficos
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# ============================================================================
# CONFIGURA√á√ÉO
# ============================================================================

# IMPORTANTE: Substitua pelo ID do seu projeto GCP
# Para obter: https://console.cloud.google.com/
BILLING_PROJECT_ID = "universal-helix-468201-g9"  # <<<< ALTERE AQUI

# Linhas que atendem o Fund√£o
LINHAS_FUNDAO = [
    '355', '371', '384', '385', '386', '387', '388', '389', '390', '391',
    '393', '394', '395', '396', '397', '398', '399', '633', '634', '635',
    '636', '638', '639', '653', '770', '771', '774', '775', '776', '777',
    '778', '779', '905', '910', '911', '917', '918'
]

# Per√≠odo de an√°lise
DATA_INICIO = '2025-03-01'
DATA_FIM = '2025-10-31'

# Threshold de atraso
ATRASO_THRESHOLD = 15  # minutos

# ============================================================================
# 1. EXTRA√á√ÉO DE DADOS
# ============================================================================

def extrair_dados_fundao(billing_project_id, teste=False):
    """
    Extrai dados das linhas do Fund√£o usando Base dos Dados
    
    Args:
        billing_project_id: ID do projeto GCP para billing
        teste: Se True, extrai apenas amostra pequena
    """
    print("=" * 80)
    print(" EXTRA√á√ÉO DE DADOS ".center(80, "="))
    print("=" * 80)
    
    linhas_str = "', '".join(LINHAS_FUNDAO)
    limite = "LIMIT 5000" if teste else ""
    
    query = f"""
    SELECT 
        data,
        servico as linha,
        datetime_partida,
        datetime_chegada,
        tempo_viagem,
        distancia_planejada,
        perc_conformidade_shape,
        perc_conformidade_registros,
        id_viagem,
        sentido
    FROM 
        `datario.transporte_rodoviario_municipal.viagem_onibus`
    WHERE 
        servico IN ('{linhas_str}')
        AND data BETWEEN '{DATA_INICIO}' AND '{DATA_FIM}'
        AND EXTRACT(DAYOFWEEK FROM data) NOT IN (1, 7)
        AND datetime_partida IS NOT NULL
        AND datetime_chegada IS NOT NULL
        AND tempo_viagem IS NOT NULL
        AND tempo_viagem > 0
    ORDER BY data, servico, datetime_partida
    {limite}
    """
    
    print(f"\nüìä Consultando Base dos Dados...")
    print(f"   Per√≠odo: {DATA_INICIO} a {DATA_FIM}")
    print(f"   Linhas: {len(LINHAS_FUNDAO)}")
    print(f"   Modo: {'TESTE (amostra)' if teste else 'COMPLETO'}")
    
    try:
        df = bd.read_sql(query, billing_project_id=billing_project_id)
        
        print(f"\n‚úÖ Dados carregados com sucesso!")
        print(f"   Total de viagens: {len(df):,}")
        print(f"   Linhas √∫nicas: {df['linha'].nunique()}")
        print(f"   Per√≠odo: {df['data'].min()} a {df['data'].max()}")
        print(f"   Mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        return df
        
    except Exception as e:
        print(f"\n‚ùå Erro ao carregar dados: {e}")
        print("\nüí° Dicas:")
        print("   1. Verifique se BILLING_PROJECT_ID est√° correto")
        print("   2. Configure: gcloud auth application-default login")
        print("   3. Ou use: bd.list_dataset_tables('datario') para listar tabelas")
        return None

# ============================================================================
# 2. PR√â-PROCESSAMENTO E C√ÅLCULO DE FEATURES
# ============================================================================

def preprocessar_dados(df):
    """
    Calcula features de atraso e velocidade
    """
    print("\n" + "=" * 80)
    print(" PR√â-PROCESSAMENTO ".center(80, "="))
    print("=" * 80)
    
    df = df.copy()
    
    # 1. Converter tempo de viagem para minutos
    df['tempo_viagem_min'] = df['tempo_viagem'] / 60.0
    
    # 2. Extrair features temporais
    df['hora_partida'] = pd.to_datetime(df['datetime_partida']).dt.hour
    df['dia_semana'] = pd.to_datetime(df['datetime_partida']).dt.dayofweek
    df['mes'] = pd.to_datetime(df['data']).dt.month
    
    # Classificar per√≠odo do dia
    def classificar_periodo(hora):
        if 6 <= hora < 9:
            return 'pico_manha'
        elif 17 <= hora < 20:
            return 'pico_tarde'
        else:
            return 'fora_pico'
    
    df['periodo'] = df['hora_partida'].apply(classificar_periodo)
    
    # 3. Calcular velocidade m√©dia (km/h)
    df['velocidade_kmh'] = np.where(
        df['tempo_viagem_min'] > 0,
        (df['distancia_planejada'] / 1000) / (df['tempo_viagem_min'] / 60),
        0
    )
    
    # 4. Calcular tempo de refer√™ncia (baseline) por linha/hora/dia_semana
    print("\nüìê Calculando tempo de refer√™ncia (baseline)...")
    tempo_ref = df.groupby(['linha', 'hora_partida', 'dia_semana'])['tempo_viagem_min'].agg([
        ('tempo_referencia', 'median'),
        ('tempo_q25', lambda x: x.quantile(0.25)),
        ('tempo_q75', lambda x: x.quantile(0.75))
    ]).reset_index()
    
    df = df.merge(tempo_ref, on=['linha', 'hora_partida', 'dia_semana'], how='left')
    
    # 5. Calcular "atraso" como desvio do tempo de refer√™ncia
    df['atraso_min'] = df['tempo_viagem_min'] - df['tempo_referencia']
    df['atrasada'] = df['atraso_min'] > ATRASO_THRESHOLD
    
    # 6. Calcular variabilidade (IQR)
    df['tempo_iqr'] = df['tempo_q75'] - df['tempo_q25']
    
    # Estat√≠sticas
    print(f"\n‚úÖ Features calculadas")
    print(f"   Tempo m√©dio de viagem: {df['tempo_viagem_min'].mean():.1f} min")
    print(f"   Velocidade m√©dia: {df['velocidade_kmh'].mean():.1f} km/h")
    print(f"   Atraso m√©dio: {df['atraso_min'].mean():+.1f} min")
    print(f"   % viagens atrasadas (>{ATRASO_THRESHOLD}min): {df['atrasada'].mean()*100:.1f}%")
    
    return df

def agregar_por_linha_dia(df):
    """
    Agrega dados por linha √ó dia para an√°lise
    """
    print("\n" + "=" * 80)
    print(" AGREGA√á√ÉO LINHA √ó DIA ".center(80, "="))
    print("=" * 80)
    
    # Agrega√ß√£o
    agregado = df.groupby(['linha', 'data']).agg({
        'atraso_min': ['mean', 'std', 'max', 'min'],
        'atrasada': 'mean',
        'velocidade_kmh': ['mean', 'std'],
        'tempo_viagem_min': ['mean', 'std'],
        'tempo_iqr': 'mean',
        'perc_conformidade_shape': 'mean',
        'perc_conformidade_registros': 'mean',
        'id_viagem': 'count'
    }).reset_index()
    
    # Simplificar nomes
    agregado.columns = [
        'linha', 'data',
        'atraso_medio', 'atraso_std', 'atraso_max', 'atraso_min',
        'prop_atrasadas',
        'velocidade_media', 'velocidade_std',
        'tempo_viagem_medio', 'tempo_viagem_std',
        'variabilidade_iqr',
        'conformidade_shape', 'conformidade_registros',
        'num_viagens'
    ]
    
    # Features temporais
    agregado['dia_semana'] = pd.to_datetime(agregado['data']).dt.dayofweek
    agregado['mes'] = pd.to_datetime(agregado['data']).dt.month
    
    # Filtrar registros com poucas viagens (outliers)
    agregado = agregado[agregado['num_viagens'] >= 3]
    
    print(f"\n‚úÖ Agrega√ß√£o conclu√≠da")
    print(f"   Registros linha√ódia: {len(agregado):,}")
    print(f"   Linhas √∫nicas: {agregado['linha'].nunique()}")
    print(f"   Dias √∫nicos: {agregado['data'].nunique()}")
    
    return agregado

# ============================================================================
# 3. AN√ÅLISE EXPLORAT√ìRIA
# ============================================================================

def analise_exploratoria(df_agregado):
    """
    An√°lise explorat√≥ria com rankings e estat√≠sticas
    """
    print("\n" + "=" * 80)
    print(" AN√ÅLISE EXPLORAT√ìRIA ".center(80, "="))
    print("=" * 80)
    
    # Rankings por linha
    ranking_atraso = df_agregado.groupby('linha')['atraso_medio'].mean().sort_values(ascending=False)
    ranking_prop = df_agregado.groupby('linha')['prop_atrasadas'].mean().sort_values(ascending=False)
    ranking_velocidade = df_agregado.groupby('linha')['velocidade_media'].mean().sort_values(ascending=True)
    
    print("\nüî¥ TOP 10 LINHAS - MAIOR ATRASO M√âDIO")
    print("-" * 50)
    for i, (linha, atraso) in enumerate(ranking_atraso.head(10).items(), 1):
        print(f"  {i:2d}. Linha {linha:>4s}: {atraso:+6.1f} min")
    
    print("\nüü° TOP 10 LINHAS - MAIOR % VIAGENS ATRASADAS")
    print("-" * 50)
    for i, (linha, prop) in enumerate(ranking_prop.head(10).items(), 1):
        print(f"  {i:2d}. Linha {linha:>4s}: {prop*100:5.1f}%")
    
    print("\nüêå TOP 10 LINHAS - MENOR VELOCIDADE M√âDIA")
    print("-" * 50)
    for i, (linha, vel) in enumerate(ranking_velocidade.head(10).items(), 1):
        print(f"  {i:2d}. Linha {linha:>4s}: {vel:5.1f} km/h")
    
    # Estat√≠sticas gerais
    print("\nüìä ESTAT√çSTICAS GERAIS")
    print("-" * 50)
    print(f"  Atraso m√©dio geral: {df_agregado['atraso_medio'].mean():+.1f} min")
    print(f"  Desvio padr√£o atraso: {df_agregado['atraso_medio'].std():.1f} min")
    print(f"  Velocidade m√©dia geral: {df_agregado['velocidade_media'].mean():.1f} km/h")
    print(f"  Conformidade m√©dia: {df_agregado['conformidade_shape'].mean()*100:.1f}%")
    
    return ranking_atraso, ranking_prop, ranking_velocidade

# ============================================================================
# 4. PCA - AN√ÅLISE DE COMPONENTES PRINCIPAIS
# ============================================================================

def aplicar_pca(df_agregado, n_components=3):
    """
    Redu√ß√£o de dimensionalidade via PCA
    """
    print("\n" + "=" * 80)
    print(" PCA - AN√ÅLISE DE COMPONENTES PRINCIPAIS ".center(80, "="))
    print("=" * 80)
    
    # Selecionar features
    features = [
        'atraso_medio', 'atraso_std', 'atraso_max',
        'prop_atrasadas',
        'velocidade_media', 'velocidade_std',
        'tempo_viagem_medio', 'tempo_viagem_std',
        'variabilidade_iqr',
        'conformidade_shape', 'conformidade_registros'
    ]
    
    X = df_agregado[features].fillna(0)
    
    # Remover valores infinitos
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
    
    # Normalizar
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # PCA
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    # Vari√¢ncia explicada
    print("\nüìà VARI√ÇNCIA EXPLICADA")
    print("-" * 50)
    cumsum = 0
    for i, var in enumerate(pca.explained_variance_ratio_, 1):
        cumsum += var
        print(f"  PC{i}: {var*100:5.1f}% (acumulado: {cumsum*100:5.1f}%)")
    
    # Loadings
    print("\nüîç LOADINGS (contribui√ß√£o de cada feature)")
    print("-" * 50)
    loadings = pd.DataFrame(
        pca.components_.T,
        columns=[f'PC{i}' for i in range(1, n_components+1)],
        index=features
    )
    
    for i in range(n_components):
        print(f"\n  PC{i+1} - Principais fatores:")
        top_features = loadings[f'PC{i+1}'].abs().nlargest(3)
        for feat, _ in top_features.items():
            val = loadings.loc[feat, f'PC{i+1}']
            sinal = "+" if val > 0 else ""
            print(f"    {feat:30s}: {sinal}{val:.3f}")
    
    return X_pca, pca, scaler, X_scaled, features

# ============================================================================
# 5. K-MEANS CLUSTERING
# ============================================================================

def determinar_k_otimo(X, k_range=range(2, 8)):
    """
    M√©todo do cotovelo + silhouette para determinar k
    """
    print("\n" + "=" * 80)
    print(" DETERMINA√á√ÉO DO K √ìTIMO ".center(80, "="))
    print("=" * 80)
    
    inertias = []
    silhouettes = []
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        inertias.append(kmeans.inertia_)
        
        if k > 1:
            sil = silhouette_score(X, labels)
            silhouettes.append(sil)
        else:
            silhouettes.append(0)
        
        print(f"  k={k}: in√©rcia={kmeans.inertia_:8.2f}, silhouette={silhouettes[-1]:.3f}")
    
    # Plotar
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # M√©todo do cotovelo
    axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
    axes[0].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)
    axes[0].set_ylabel('In√©rcia', fontsize=11)
    axes[0].set_title('M√©todo do Cotovelo', fontsize=12, fontweight='bold')
    axes[0].grid(True, alpha=0.3)
    
    # Silhouette
    axes[1].plot(k_range, silhouettes, 'ro-', linewidth=2, markersize=8)
    axes[1].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)
    axes[1].set_ylabel('Silhouette Score', fontsize=11)
    axes[1].set_title('Coeficiente de Silhueta', fontsize=12, fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('k_otimo.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Sugerir k √≥timo
    k_sugerido = k_range[np.argmax(silhouettes)]
    print(f"\nüí° K sugerido (maior silhouette): {k_sugerido}")
    
    return inertias, silhouettes

def aplicar_kmeans(X, n_clusters=4):
    """
    Clusteriza√ß√£o via K-Means
    """
    print(f"\n‚öôÔ∏è  Aplicando K-Means com k={n_clusters}...")
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X)
    
    # Silhouette score
    sil_score = silhouette_score(X, clusters)
    
    print(f"‚úÖ Clustering conclu√≠do")
    print(f"   Silhouette Score: {sil_score:.3f}")
    
    # Distribui√ß√£o
    unique, counts = np.unique(clusters, return_counts=True)
    print(f"\n   Distribui√ß√£o de clusters:")
    for cluster_id, count in zip(unique, counts):
        print(f"     Cluster {cluster_id}: {count:5d} registros ({count/len(clusters)*100:4.1f}%)")
    
    return clusters, kmeans

def analisar_clusters(df_agregado, clusters):
    """
    Caracteriza cada cluster identificando perfis
    """
    print("\n" + "=" * 80)
    print(" CARACTERIZA√á√ÉO DOS CLUSTERS ".center(80, "="))
    print("=" * 80)
    
    df_agregado = df_agregado.copy()
    df_agregado['cluster'] = clusters
    
    resultados_clusters = []
    
    for cluster_id in sorted(df_agregado['cluster'].unique()):
        print(f"\n{'='*80}")
        print(f" CLUSTER {cluster_id} ".center(80, "="))
        print('='*80)
        
        cluster_df = df_agregado[df_agregado['cluster'] == cluster_id]
        
        # Tamanho
        print(f"\nüìä Tamanho: {len(cluster_df):,} registros ({len(cluster_df)/len(df_agregado)*100:.1f}%)")
        
        # Linhas
        linhas = sorted(cluster_df['linha'].unique())
        print(f"\nüöå Linhas ({len(linhas)}): {', '.join(linhas)}")
        
        # Caracter√≠sticas m√©dias
        stats = {
            'atraso_medio': cluster_df['atraso_medio'].mean(),
            'prop_atrasadas': cluster_df['prop_atrasadas'].mean(),
            'velocidade_media': cluster_df['velocidade_media'].mean(),
            'conformidade': cluster_df['conformidade_shape'].mean(),
            'variabilidade': cluster_df['atraso_std'].mean()
        }
        
        print(f"\nüìà Caracter√≠sticas m√©dias:")
        print(f"   Atraso m√©dio:       {stats['atraso_medio']:+6.1f} min")
        print(f"   % atrasadas:        {stats['prop_atrasadas']*100:6.1f}%")
        print(f"   Velocidade:         {stats['velocidade_media']:6.1f} km/h")
        print(f"   Conformidade:       {stats['conformidade']*100:6.1f}%")
        print(f"   Variabilidade:      {stats['variabilidade']:6.1f} min")
        
        # Identificar perfil
        if stats['atraso_medio'] > 10 and stats['prop_atrasadas'] > 0.5:
            perfil = "üî¥ CRONICAMENTE ATRASADO - Alta irregularidade"
        elif stats['atraso_medio'] > 5 and stats['prop_atrasadas'] > 0.3:
            perfil = "üü° MODERADAMENTE IRREGULAR - Atrasos frequentes"
        elif stats['velocidade_media'] < 15:
            perfil = "üêå LENTO MAS REGULAR - Baixa velocidade operacional"
        elif stats['atraso_medio'] < 0:
            perfil = "üü¢ PONTUAL E R√ÅPIDO - Acima da expectativa"
        else:
            perfil = "üü¢ REGULAR E PREVIS√çVEL - Opera√ß√£o normal"
        
        print(f"\nüè∑Ô∏è  Perfil: {perfil}")
        
        resultados_clusters.append({
            'cluster': cluster_id,
            'perfil': perfil,
            'linhas': linhas,
            **stats
        })
    
    return pd.DataFrame(resultados_clusters)

# ============================================================================
# 6. VISUALIZA√á√ïES
# ============================================================================

def visualizar_pca_clusters(X_pca, clusters, df_agregado):
    """
    Visualiza clusters no espa√ßo PCA
    """
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # PC1 vs PC2
    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], 
                               c=clusters, cmap='viridis', 
                               alpha=0.6, s=30, edgecolors='black', linewidth=0.3)
    axes[0].set_xlabel('PC1 (Componente Principal 1)', fontsize=11)
    axes[0].set_ylabel('PC2 (Componente Principal 2)', fontsize=11)
    axes[0].set_title('Clusters no Espa√ßo PCA (PC1 vs PC2)', fontsize=13, fontweight='bold')
    axes[0].grid(True, alpha=0.3)
    cbar1 = plt.colorbar(scatter1, ax=axes[0])
    cbar1.set_label('Cluster', fontsize=10)
    
    # PC1 vs PC3
    if X_pca.shape[1] > 2:
        scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 2], 
                                   c=clusters, cmap='viridis', 
                                   alpha=0.6, s=30, edgecolors='black', linewidth=0.3)
        axes[1].set_xlabel('PC1 (Componente Principal 1)', fontsize=11)
        axes[1].set_ylabel('PC3 (Componente Principal 3)', fontsize=11)
        axes[1].set_title('Clusters no Espa√ßo PCA (PC1 vs PC3)', fontsize=13, fontweight='bold')
        axes[1].grid(True, alpha=0.3)
        cbar2 = plt.colorbar(scatter2, ax=axes[1])
        cbar2.set_label('Cluster', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('pca_clusters.png', dpi=300, bbox_inches='tight')
    plt.show()

def visualizar_rankings(ranking_atraso, ranking_prop, ranking_velocidade):
    """
    Visualiza rankings das piores linhas
    """
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # Atraso m√©dio
    top_atraso = ranking_atraso.head(12)
    axes[0].barh(range(len(top_atraso)), top_atraso.values, 
                 color='crimson', alpha=0.7, edgecolor='black', linewidth=0.5)
    axes[0].set_yticks(range(len(top_atraso)))
    axes[0].set_yticklabels(top_atraso.index)
    axes[0].set_xlabel('Atraso M√©dio (minutos)', fontsize=10)
    axes[0].set_ylabel('Linha', fontsize=10)
    axes[0].set_title('Top 12 - Maior Atraso M√©dio', fontsize=11, fontweight='bold')
    axes[0].grid(True, alpha=0.3, axis='x')
    axes[0].invert_yaxis()
    
    # % atrasadas
    top_prop = ranking_prop.head(12)
    axes[1].barh(range(len(top_prop)), top_prop.values * 100, 
                 color='orange', alpha=0.7, edgecolor='black', linewidth=0.5)
    axes[1].set_yticks(range(len(top_prop)))
    axes[1].set_yticklabels(top_prop.index)
    axes[1].set_xlabel('% Viagens Atrasadas', fontsize=10)
    axes[1].set_ylabel('Linha', fontsize=10)
    axes[1].set_title('Top 12 - Maior % de Atrasos', fontsize=11, fontweight='bold')
    axes[1].grid(True, alpha=0.3, axis='x')
    axes[1].invert_yaxis()
    
    # Velocidade
    top_vel = ranking_velocidade.head(12)
    axes[2].barh(range(len(top_vel)), top_vel.values, 
                 color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)
    axes[2].set_yticks(range(len(top_vel)))
    axes[2].set_yticklabels(top_vel.index)
    axes[2].set_xlabel('Velocidade M√©dia (km/h)', fontsize=10)
    axes[2].set_ylabel('Linha', fontsize=10)
    axes[2].set_title('Top 12 - Menor Velocidade', fontsize=11, fontweight='bold')
    axes[2].grid(True, alpha=0.3, axis='x')
    axes[2].invert_yaxis()
    
    plt.tight_layout()
    plt.savefig('rankings.png', dpi=300, bbox_inches='tight')
    plt.show()

# ============================================================================
# 7. PIPELINE COMPLETO
# ============================================================================

def pipeline_completo(billing_project_id, teste=False):
    """
    Pipeline completo de an√°lise
    
    Args:
        billing_project_id: ID do projeto GCP
        teste: Se True, usa amostra pequena
    """
    print("\n" + "=" * 80)
    print("")
    print(" PROJETO COCADA 2025-2 ".center(80, "="))
    print(" An√°lise de Atrasos - Linhas do Fund√£o ".center(80))
    print(" Jo√£o Victor Borges Nascimento - 121064604 ".center(80))
    print("")
    print("=" * 80)
    
    # 1. Extra√ß√£o
    df = extrair_dados_fundao(billing_project_id, teste=teste)
    if df is None:
        return None
    
    # 2. Pr√©-processamento
    df = preprocessar_dados(df)
    
    # 3. Agrega√ß√£o
    df_agregado = agregar_por_linha_dia(df)
    
    # 4. An√°lise Explorat√≥ria
    ranking_atraso, ranking_prop, ranking_vel = analise_exploratoria(df_agregado)
    
    # 5. PCA
    X_pca, pca, scaler, X_scaled, features = aplicar_pca(df_agregado, n_components=3)
    
    # 6. Determinar k √≥timo
    inertias, silhouettes = determinar_k_otimo(X_scaled, k_range=range(2, 7))
    
    # 7. K-Means (usar k=4 ou ajustar baseado no silhouette)
    clusters, kmeans = aplicar_kmeans(X_scaled, n_clusters=4)
    
    # 8. An√°lise de Clusters
    df_clusters = analisar_clusters(df_agregado, clusters)
    
    # 9. Visualiza√ß√µes
    print("\nüìä Gerando visualiza√ß√µes...")
    visualizar_pca_clusters(X_pca, clusters, df_agregado)
    visualizar_rankings(ranking_atraso, ranking_prop, ranking_vel)
    
    # 10. Resultados finais
    print("\n" + "=" * 80)
    print(" AN√ÅLISE CONCLU√çDA ".center(80, "="))
    print("=" * 80)
    print("\n‚úÖ Arquivos gerados:")
    print("   - pca_clusters.png")
    print("   - k_otimo.png")
    print("   - pca_clusters.png")
    print("   - rankings.png")
    
    # Salvar resultados em CSV
    df_agregado['cluster'] = clusters
    df_agregado.to_csv('resultados_agregados.csv', index=False)
    df_clusters.to_csv('caracterizacao_clusters.csv', index=False)
    
    print("   - resultados_agregados.csv")
    print("   - caracterizacao_clusters.csv")
    
    print("\nüìã Resumo dos Resultados:")
    print(f"   Total de viagens analisadas: {len(df):,}")
    print(f"   Linhas com dados completos: {df_agregado['linha'].nunique()}")
    print(f"   Clusters identificados: {len(df_clusters)}")
    print(f"   Per√≠odo analisado: {DATA_INICIO} a {DATA_FIM}")
    
    return {
        'df_original': df,
        'df_agregado': df_agregado,
        'X_pca': X_pca,
        'clusters': clusters,
        'pca': pca,
        'kmeans': kmeans,
        'rankings': (ranking_atraso, ranking_prop, ranking_vel),
        'df_clusters': df_clusters
    }

# Teste r√°pido (5000 viagens)
resultados = pipeline_completo("universal-helix-468201-g9", teste=True)

# An√°lise completa (todos os dados)
# resultados = pipeline_completo("universal-helix-468201-g9", teste=False)